{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":441417,"sourceType":"datasetVersion","datasetId":200079},{"sourceId":1355361,"sourceType":"datasetVersion","datasetId":789090},{"sourceId":7008448,"sourceType":"datasetVersion","datasetId":4029201}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-26T18:33:39.400234Z","iopub.execute_input":"2023-11-26T18:33:39.400925Z","iopub.status.idle":"2023-11-26T18:33:39.411776Z","shell.execute_reply.started":"2023-11-26T18:33:39.400883Z","shell.execute_reply":"2023-11-26T18:33:39.410701Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv\n/kaggle/input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv\n/kaggle/input/nlp-project-datset-assist/english__summarized\n/kaggle/input/nlp-project-datset-assist/ckpt-2.data-00000-of-00001\n/kaggle/input/nlp-project-datset-assist/ckpt-2.index\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport ast\n\n# Assuming you've read the CSV file into a DataFrame\ndata = pd.read_csv('/kaggle/input/nlp-project-datset-assist/english__summarized')\n\n# Define the function to convert string representation to a normal string\ndef get_normal_str(text):\n    text_list = ast.literal_eval(text)\n    normal_string = text_list[0]\n    return normal_string\n\n# Apply the function to the 'english_summaries' column\ndata['normal_summary'] = data['english_summaries'].apply(get_normal_str)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T18:55:26.387323Z","iopub.execute_input":"2023-11-26T18:55:26.387700Z","iopub.status.idle":"2023-11-26T18:55:26.634619Z","shell.execute_reply.started":"2023-11-26T18:55:26.387663Z","shell.execute_reply":"2023-11-26T18:55:26.633283Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# !pip install ipywidgets\n# !pip install -U accelerate\n!pip install -U accelerate\n# !pip install -U transformers\n# !pip install transformers[torch]\n# !pip install accelerate -U\n!pip install sacrebleu\n!pip install evaluate\n!pip install rouge_score\n# !pip install fasteda","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-26T18:55:26.636344Z","iopub.execute_input":"2023-11-26T18:55:26.637009Z","iopub.status.idle":"2023-11-26T18:56:18.444938Z","shell.execute_reply.started":"2023-11-26T18:55:26.636975Z","shell.execute_reply":"2023-11-26T18:56:18.443336Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.22.0)\nCollecting accelerate\n  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0+cpu)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.16.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.22.0\n    Uninstalling accelerate-0.22.0:\n      Successfully uninstalled accelerate-0.22.0\nSuccessfully installed accelerate-0.24.1\nCollecting sacrebleu\n  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.6.3)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.23.5)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.3)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.8.2 sacrebleu-2.3.2\nCollecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (9.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=96c0f445b1c93312621e9d25c8ec16c213c00c720fc328cb896f2e45a4034060\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q seaborn","metadata":{"execution":{"iopub.status.busy":"2023-11-26T18:56:18.447294Z","iopub.execute_input":"2023-11-26T18:56:18.447677Z","iopub.status.idle":"2023-11-26T18:56:29.841074Z","shell.execute_reply.started":"2023-11-26T18:56:18.447641Z","shell.execute_reply":"2023-11-26T18:56:29.839190Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.executable","metadata":{"execution":{"iopub.status.busy":"2023-11-26T18:56:29.843070Z","iopub.execute_input":"2023-11-26T18:56:29.843454Z","iopub.status.idle":"2023-11-26T18:56:29.850622Z","shell.execute_reply.started":"2023-11-26T18:56:29.843421Z","shell.execute_reply":"2023-11-26T18:56:29.849533Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'/opt/conda/bin/python3.10'"},"metadata":{}}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\n\nfrom transformers import TFAutoModelForSeq2SeqLM\nfrom transformers import DataCollatorForSeq2Seq\n\nimport transformers\nimport accelerate\nimport evaluate\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-11-26T18:56:29.852649Z","iopub.execute_input":"2023-11-26T18:56:29.852946Z","iopub.status.idle":"2023-11-26T18:56:47.464009Z","shell.execute_reply.started":"2023-11-26T18:56:29.852920Z","shell.execute_reply":"2023-11-26T18:56:47.462762Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# data = pd.read_csv('../input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv',encoding='utf-8')\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:16:09.908202Z","iopub.execute_input":"2023-11-20T08:16:09.908922Z","iopub.status.idle":"2023-11-20T08:16:10.822343Z","shell.execute_reply.started":"2023-11-20T08:16:09.908892Z","shell.execute_reply":"2023-11-20T08:16:10.821349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=data['source'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:08:25.894930Z","iopub.execute_input":"2023-11-20T08:08:25.895915Z","iopub.status.idle":"2023-11-20T08:08:26.140261Z","shell.execute_reply.started":"2023-11-20T08:08:25.895880Z","shell.execute_reply":"2023-11-20T08:08:26.139486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## counting length of english and hindi sentence\ndata['english_sentence'] = data['english_sentence'].fillna('')\n\n# Count the length of English and Hindi sentences\ndata['english_length'] = data['english_sentence'].apply(lambda x: len(x.split(' ')))\ndata['hindi_length'] = data['hindi_sentence'].apply(lambda x: len(x.split(' ')))\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:08:28.845675Z","iopub.execute_input":"2023-11-20T08:08:28.846060Z","iopub.status.idle":"2023-11-20T08:08:29.202909Z","shell.execute_reply.started":"2023-11-20T08:08:28.846033Z","shell.execute_reply":"2023-11-20T08:08:29.202204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group the data by 'source' and calculate average lengths for each group\naverage_lengths = data.groupby('source')[['english_length', 'hindi_length']].mean().reset_index()\n\n# Print the resulting DataFrame\nprint(average_lengths)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:08:29.280649Z","iopub.execute_input":"2023-11-20T08:08:29.281475Z","iopub.status.idle":"2023-11-20T08:08:29.314157Z","shell.execute_reply.started":"2023-11-20T08:08:29.281428Z","shell.execute_reply":"2023-11-20T08:08:29.312419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fasteda import fast_eda\n\nfast_eda(data[['english_length', 'hindi_length']])","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:08:29.581981Z","iopub.execute_input":"2023-11-20T08:08:29.582417Z","iopub.status.idle":"2023-11-20T08:08:36.750789Z","shell.execute_reply.started":"2023-11-20T08:08:29.582392Z","shell.execute_reply":"2023-11-20T08:08:36.749399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:08:40.585021Z","iopub.execute_input":"2023-11-20T08:08:40.586137Z","iopub.status.idle":"2023-11-20T08:08:40.591235Z","shell.execute_reply.started":"2023-11-20T08:08:40.586088Z","shell.execute_reply":"2023-11-20T08:08:40.589899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-11-26T18:58:27.252788Z","iopub.execute_input":"2023-11-26T18:58:27.253903Z","iopub.status.idle":"2023-11-26T18:58:27.273707Z","shell.execute_reply.started":"2023-11-26T18:58:27.253850Z","shell.execute_reply":"2023-11-26T18:58:27.272615Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"      Unnamed: 0 source                                   english_sentence  \\\n0              0  tides  32 . People abroad can get advice from a Gover...   \n1              1  tides  Her fear of flying has forced her to travel in...   \n2              2  tides  There was accumulation of stocks at the pithea...   \n3              3  tides  This revolution is finished in 30 muhuita , i....   \n4              4  tides              The woman triumphs by her suffering .   \n...          ...    ...                                                ...   \n9995        9995  tides  Besides statements pointing out mistakes or in...   \n9996        9996  tides        Attempts to play down the loss have begun .   \n9997        9997  tides  Korean car manufacturer Hyundai though is not ...   \n9998        9998  tides  He demanded that India should be granted an au...   \n9999        9999  tides  But what is left to me here what ambition or m...   \n\n                                         hindi_sentence  \\\n0     32 विदेश में व्यक्ति वहाँ ब्रिटीश ऐम्बैसी , का...   \n1     ड़ौर के कारण उन्हें घरेलू उड़नों में अक्सर कॉक...   \n2     सन् 1976-77 के अंत तक 1 करोड़ 46 लाख टन तक का ...   \n3     यह परिक्रमा 30 मुहुर्त अर्थात एक नक़्तंदिव में...   \n4         स्त्री अपनी यातना में भी , पराजित नहीं होती .   \n...                                                 ...   \n9995  किसी मंत्री द्वारा या अन्य सदस्य द्वारा दिए गए...   \n9996  इसके महत्व को भी कम करके आंकने के प्रयास शुरू ...   \n9997  मगर कोरियाई कार निर्माता हंड़ई समज्हैते के मुत...   \n9998  इसके विपरीत उन्होंने ब्रिटिश प्रभुत्व से पूर्ण...   \n9999  इस सुनसान और उजाड़ जीवन में भला कौन-सी महत्वाक...   \n\n                                      english_summaries  \\\n0     ['People abroad can get advice from a Governme...   \n1     ['Her fear of flying has forced her to travel ...   \n2     ['There was accumulation of stocks at the pith...   \n3     ['This revolution is finished in 30 muhuita, i...   \n4     ['The woman triumphs by her suffering.<n>The w...   \n...                                                 ...   \n9995  ['Other items of business which fall under thi...   \n9996  ['Attempts to play down the loss have begun.<n...   \n9997  ['Korean car manufacturer Hyundai though is no...   \n9998  [\"He demanded that India should be granted an ...   \n9999  ['What is left to me here what ambition or mis...   \n\n                                         normal_summary  \n0     People abroad can get advice from a Government...  \n1     Her fear of flying has forced her to travel in...  \n2     There was accumulation of stocks at the pithea...  \n3     This revolution is finished in 30 muhuita, i.e...  \n4     The woman triumphs by her suffering.<n>The wom...  \n...                                                 ...  \n9995  Other items of business which fall under this ...  \n9996  Attempts to play down the loss have begun.<n>T...  \n9997  Korean car manufacturer Hyundai though is not ...  \n9998  He demanded that India should be granted an au...  \n9999  What is left to me here what ambition or missi...  \n\n[10000 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>source</th>\n      <th>english_sentence</th>\n      <th>hindi_sentence</th>\n      <th>english_summaries</th>\n      <th>normal_summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>tides</td>\n      <td>32 . People abroad can get advice from a Gover...</td>\n      <td>32 विदेश में व्यक्ति वहाँ ब्रिटीश ऐम्बैसी , का...</td>\n      <td>['People abroad can get advice from a Governme...</td>\n      <td>People abroad can get advice from a Government...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>tides</td>\n      <td>Her fear of flying has forced her to travel in...</td>\n      <td>ड़ौर के कारण उन्हें घरेलू उड़नों में अक्सर कॉक...</td>\n      <td>['Her fear of flying has forced her to travel ...</td>\n      <td>Her fear of flying has forced her to travel in...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>tides</td>\n      <td>There was accumulation of stocks at the pithea...</td>\n      <td>सन् 1976-77 के अंत तक 1 करोड़ 46 लाख टन तक का ...</td>\n      <td>['There was accumulation of stocks at the pith...</td>\n      <td>There was accumulation of stocks at the pithea...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>tides</td>\n      <td>This revolution is finished in 30 muhuita , i....</td>\n      <td>यह परिक्रमा 30 मुहुर्त अर्थात एक नक़्तंदिव में...</td>\n      <td>['This revolution is finished in 30 muhuita, i...</td>\n      <td>This revolution is finished in 30 muhuita, i.e...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>tides</td>\n      <td>The woman triumphs by her suffering .</td>\n      <td>स्त्री अपनी यातना में भी , पराजित नहीं होती .</td>\n      <td>['The woman triumphs by her suffering.&lt;n&gt;The w...</td>\n      <td>The woman triumphs by her suffering.&lt;n&gt;The wom...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>9995</td>\n      <td>tides</td>\n      <td>Besides statements pointing out mistakes or in...</td>\n      <td>किसी मंत्री द्वारा या अन्य सदस्य द्वारा दिए गए...</td>\n      <td>['Other items of business which fall under thi...</td>\n      <td>Other items of business which fall under this ...</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>9996</td>\n      <td>tides</td>\n      <td>Attempts to play down the loss have begun .</td>\n      <td>इसके महत्व को भी कम करके आंकने के प्रयास शुरू ...</td>\n      <td>['Attempts to play down the loss have begun.&lt;n...</td>\n      <td>Attempts to play down the loss have begun.&lt;n&gt;T...</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>9997</td>\n      <td>tides</td>\n      <td>Korean car manufacturer Hyundai though is not ...</td>\n      <td>मगर कोरियाई कार निर्माता हंड़ई समज्हैते के मुत...</td>\n      <td>['Korean car manufacturer Hyundai though is no...</td>\n      <td>Korean car manufacturer Hyundai though is not ...</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>9998</td>\n      <td>tides</td>\n      <td>He demanded that India should be granted an au...</td>\n      <td>इसके विपरीत उन्होंने ब्रिटिश प्रभुत्व से पूर्ण...</td>\n      <td>[\"He demanded that India should be granted an ...</td>\n      <td>He demanded that India should be granted an au...</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>9999</td>\n      <td>tides</td>\n      <td>But what is left to me here what ambition or m...</td>\n      <td>इस सुनसान और उजाड़ जीवन में भला कौन-सी महत्वाक...</td>\n      <td>['What is left to me here what ambition or mis...</td>\n      <td>What is left to me here what ambition or missi...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_req = data.loc[data['source']=='tides']\n\ndata_new = Dataset.from_pandas( data_req )\ndata_new = data_new.train_test_split(test_size=0.15)\ndata_new\n\ndef preprocess_function(data):\n    inputs = [ex for ex in data[\"normal_summary\"]]\n    targets = [ex for ex in data[\"hindi_sentence\"]]\n    model_inputs = tokenizer( inputs, text_target=targets, max_length=max_length, truncation=True )\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-11-26T18:58:36.945013Z","iopub.execute_input":"2023-11-26T18:58:36.945452Z","iopub.status.idle":"2023-11-26T18:58:37.053564Z","shell.execute_reply.started":"2023-11-26T18:58:36.945416Z","shell.execute_reply":"2023-11-26T18:58:37.052316Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data_new[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-11-26T18:58:45.694959Z","iopub.execute_input":"2023-11-26T18:58:45.695391Z","iopub.status.idle":"2023-11-26T18:58:45.705754Z","shell.execute_reply.started":"2023-11-26T18:58:45.695351Z","shell.execute_reply":"2023-11-26T18:58:45.704852Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'Unnamed: 0': 5100,\n 'source': 'tides',\n 'english_sentence': 'Subhas Chandra had made out his plans of an armed thrust into India from the North-west on the basis of the status-quo of non-aggression being maintained between Germany and Soviet Russia .',\n 'hindi_sentence': 'सुभाष चन्द्र ने भारत में उत्तर-पश्चिम से सशस्त्र प्रवेश की योजना बनायी थी , जिसका आधार यह था कि जर्मनी और सोवियत रूस में अनाक्रमण की स्थिति बनी रहेगी .',\n 'english_summaries': \"['Subhas Chandra had made out his plans of an armed thrust into India from the North-west on the basis of the status-quo of non-aggression being maintained between Germany and Soviet Russia.']\",\n 'normal_summary': 'Subhas Chandra had made out his plans of an armed thrust into India from the North-west on the basis of the status-quo of non-aggression being maintained between Germany and Soviet Russia.',\n '__index_level_0__': 5100}"},"metadata":{}}]},{"cell_type":"code","source":"max_length = 128\nnum_epochs = 10\nbatch_size = 16\n\nmetric_bleu = evaluate.load(\"sacrebleu\")\nmetric_rouge = evaluate.load(\"rouge\")\nmetrics = [metric_bleu, metric_rouge]\nmetrics_name = ['sacrebleu', 'rouge']","metadata":{"execution":{"iopub.status.busy":"2023-11-26T18:58:55.259011Z","iopub.execute_input":"2023-11-26T18:58:55.259410Z","iopub.status.idle":"2023-11-26T18:58:59.515082Z","shell.execute_reply.started":"2023-11-26T18:58:55.259379Z","shell.execute_reply":"2023-11-26T18:58:59.514135Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68cb8617d7f843ff8977598a31dc74c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160e4ab9f18c4be2bdc3eb8c2281c3dc"}},"metadata":{}}]},{"cell_type":"code","source":"@tf.function(jit_compile=True)\ndef generate_with_xla(model, batch):\n    return model.generate( input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], max_new_tokens=128 )\n\ndef compute_metrics(model):\n    all_preds = []\n    all_labels = []\n    result={}\n    for batch, labels in tqdm(tf_eval_dataset):\n        predictions = generate_with_xla(model, batch)\n        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n        labels = labels.numpy()\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        decoded_preds = [pred.strip() for pred in decoded_preds]\n        decoded_labels = [[label.strip()] for label in decoded_labels]\n        all_preds.extend(decoded_preds)\n        all_labels.extend(decoded_labels)\n    \n    for i, metric in enumerate(metrics):\n        result[metrics_name[i]] = metric.compute(predictions=all_preds, references=all_labels)\n    \n    return result\n\nfrom huggingface_hub import notebook_login, login\nnotebook_login()\nlogin(token=\"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\", write_permission=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T18:58:59.516724Z","iopub.execute_input":"2023-11-26T18:58:59.518232Z","iopub.status.idle":"2023-11-26T18:59:00.210130Z","shell.execute_reply.started":"2023-11-26T18:58:59.518167Z","shell.execute_reply":"2023-11-26T18:59:00.209299Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b17ef0593e6469f85b6f701a487baf2"}},"metadata":{}},{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install -U transformers[sentencepiece]\n# !pip install -q sentencepiece","metadata":{"execution":{"iopub.status.busy":"2023-11-14T22:56:28.895383Z","iopub.execute_input":"2023-11-14T22:56:28.895874Z","iopub.status.idle":"2023-11-14T22:56:28.901487Z","shell.execute_reply.started":"2023-11-14T22:56:28.895827Z","shell.execute_reply":"2023-11-14T22:56:28.900282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import sentencepiece\nmodel_checkpoints = [\"Helsinki-NLP/opus-mt-en-hi\", \"facebook/mbart-large-50-many-to-many-mmt\", \"t5-small\", \"xlm-roberta-base\", 'anjankumar/Anjan-finetuned-iitbombay-en-to-hi']\nscores = {}\nfor model_checkpoint in model_checkpoints:\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\", use_fast=False)\n        tokenized_datasets = data_new.map(preprocess_function, batched=True, remove_columns=data_new[\"train\"].column_names)\n        model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)\n\n        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)\n        tf_train_dataset = model.prepare_tf_dataset( tokenized_datasets[\"train\"], collate_fn=data_collator, shuffle=True, batch_size=batch_size )\n        tf_eval_dataset = model.prepare_tf_dataset( tokenized_datasets[\"test\"], collate_fn=data_collator, shuffle=False, batch_size=batch_size )\n        \n        scores[model_checkpoint] = compute_metrics(model)\n        \n    except Exception as e:\n        print(f\"Error processing model '{model_checkpoint}': {str(e)}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-26T18:59:03.821767Z","iopub.execute_input":"2023-11-26T18:59:03.822450Z","iopub.status.idle":"2023-11-26T21:04:12.450059Z","shell.execute_reply.started":"2023-11-26T18:59:03.822413Z","shell.execute_reply":"2023-11-26T21:04:12.449158Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ffb706b1bd040058ebda394633ea11d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbce1f94d2824a29ae1d7e060ff6cac7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading source.spm:   0%|          | 0.00/812k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3673dda792d4d3aa7a1bec9eda74ed7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading target.spm:   0%|          | 0.00/1.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e12fb937c6f4e9ab1bb35af5e4c5754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b27175c85fad44abb5cd42b406826839"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea0c6607f0b9486c9ed7ad97a7ae2f41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2a06a52de8e410892572f81919ce9b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e5924644f04e63bd400ead0109c077"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFMarianMTModel.\n\nAll the weights of TFMarianMTModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n100%|██████████| 94/94 [52:07<00:00, 33.28s/it] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c877010e3854d8cb2eee68bd261bb5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"647f7df9ce0a4848bea0bb4d8703ed67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb768810de8e4a4ab6f20c941da3d365"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"260d20f6e56b45abb614159481d5a7e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4543da0443f48b09085619cfcc370cc"}},"metadata":{}},{"name":"stdout","text":"Error processing model 'facebook/mbart-large-50-many-to-many-mmt': None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91b34bda59d645568dc9e03eba4481a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03acb4929a124ded9fe67602ad5eb0c3"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd1a4a88cec24c688728b14ec7700e5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fad07c6dd4c43438b33daae0dee0dc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"777f380f28cb484192da9c806cdab012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8daea75dd9fd465cbb2d9e140547f299"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n\nAll the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n100%|██████████| 94/94 [18:38<00:00, 11.89s/it]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae4bb3f7f4c465d8de4f35552fb4c7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ba6ba1c401d478593c17ef8743f8b35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74f80598940343c5b5a2be1a535bc0a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67c9bba42e74ed4876a321bb90cc12e"}},"metadata":{}},{"name":"stdout","text":"Error processing model 'xlm-roberta-base': Unrecognized configuration class <class 'transformers.models.xlm_roberta.configuration_xlm_roberta.XLMRobertaConfig'> for this kind of AutoModel: TFAutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, LEDConfig, MarianConfig, MBartConfig, MT5Config, PegasusConfig, T5Config.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/351 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b26308a0e8ec420ba691f7bf38f8c7c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading source.spm:   0%|          | 0.00/812k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"162356bd57bc478b90ac918ac54f4ebe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading target.spm:   0%|          | 0.00/1.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db135019058e4022b89a35cc58509106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8045ccaf879e4732998635fffd14a57a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1082d3d6f3194572a615487ee56ed6f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8823aa05a145099ab489623907a37c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0723467cadd4e4cab41cc6f53afe2ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bb946e9645e4c16af15cb589c2b8224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/304M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcda46f0f0d1442c828ec117b5c01d94"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMarianMTModel: ['lm_head.weight']\n- This IS expected if you are initializing TFMarianMTModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFMarianMTModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFMarianMTModel were not initialized from the PyTorch model and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 94/94 [52:23<00:00, 33.44s/it] \n","output_type":"stream"}]},{"cell_type":"code","source":"scores","metadata":{"execution":{"iopub.status.busy":"2023-11-26T21:04:39.487102Z","iopub.execute_input":"2023-11-26T21:04:39.487555Z","iopub.status.idle":"2023-11-26T21:04:39.497453Z","shell.execute_reply.started":"2023-11-26T21:04:39.487518Z","shell.execute_reply":"2023-11-26T21:04:39.496127Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'Helsinki-NLP/opus-mt-en-hi': {'sacrebleu': {'score': 4.585965825427612,\n   'counts': [13315, 4112, 1464, 540],\n   'totals': [58206, 56706, 55206, 53706],\n   'precisions': [22.875648558567846,\n    7.2514372376820795,\n    2.6518856646016737,\n    1.0054742486872974],\n   'bp': 1.0,\n   'sys_len': 58206,\n   'ref_len': 39056},\n  'rouge': {'rouge1': 0.05655351217410044,\n   'rouge2': 0.012887434592697748,\n   'rougeL': 0.05596819367423084,\n   'rougeLsum': 0.05625506318014058}},\n 't5-small': {'sacrebleu': {'score': 0.11866202020299058,\n   'counts': [2152, 97, 34, 6],\n   'totals': [70337, 68840, 67350, 65862],\n   'precisions': [3.059556136883859,\n    0.14090644973852412,\n    0.050482553823311065,\n    0.009109957183201239],\n   'bp': 1.0,\n   'sys_len': 70337,\n   'ref_len': 4380},\n  'rouge': {'rouge1': 0.01100867014978436,\n   'rouge2': 0.002257577389592094,\n   'rougeL': 0.01050458248999634,\n   'rougeLsum': 0.010467168224673367}},\n 'anjankumar/Anjan-finetuned-iitbombay-en-to-hi': {'sacrebleu': {'score': 4.573636538959864,\n   'counts': [13604, 4164, 1491, 563],\n   'totals': [59690, 58190, 56690, 55190],\n   'precisions': [22.79108728430223,\n    7.155868705963224,\n    2.6300934909155056,\n    1.0201123391918825],\n   'bp': 1.0,\n   'sys_len': 59690,\n   'ref_len': 39056},\n  'rouge': {'rouge1': 0.054830287740581886,\n   'rouge2': 0.011643565693565693,\n   'rougeL': 0.054115058242793,\n   'rougeLsum': 0.05420153403623476}}}"},"metadata":{}}]},{"cell_type":"code","source":"@tf.function(jit_compile=True)\ndef generate_with_xla(model, batch):\n    return model.generate( input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], max_new_tokens=128 )\n\ndef compute_metrics(model):\n    all_preds = []\n    all_labels = []\n    result={}\n    for batch, labels in tqdm(tf_eval_dataset):\n        predictions = generate_with_xla(model, batch)\n        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n        labels = labels.numpy()\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        decoded_preds = [pred.strip() for pred in decoded_preds]\n        decoded_labels = [[label.strip()] for label in decoded_labels]\n        all_preds.extend(decoded_preds)\n        all_labels.extend(decoded_labels)\n    \n    for i, metric in enumerate(metrics):\n        result[metrics_name[i]] = metric.compute(predictions=all_preds, references=all_labels)\n    \n    return result\n\nfrom huggingface_hub import notebook_login, login\nnotebook_login()\nlogin(token=\"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\", write_permission=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T01:07:52.282569Z","iopub.execute_input":"2023-11-15T01:07:52.283359Z","iopub.status.idle":"2023-11-15T01:07:52.828277Z","shell.execute_reply.started":"2023-11-15T01:07:52.283308Z","shell.execute_reply":"2023-11-15T01:07:52.826822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nmax_length = 128  # Adjust as needed\ninputs = [ex for ex in data[\"english_sentence\"]]\ntargets = [ex for ex in data[\"hindi_sentence\"]]\n\ntgt_lang = \"hi_IN\"\ntokenizer = MBart50TokenizerFast.from_pretrained(model_checkpoint, src_lang=\"en_XX\", tgt_lang=\"hi_IN\")\nmodel_inputs = tokenizer(inputs, text_target=targets, return_tensors=\"pt\")\n\n@tf.function(jit_compile=True)\ndef generate_with_xla(model, batch):\n    return model.generate( input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], max_new_tokens=128 )\n\ndef compute_metrics(model):\n    all_preds = []\n    all_labels = []\n    result={}\n    for batch, labels in tqdm(tf_eval_dataset):\n        predictions = generate_with_xla(model, batch)\n        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n        labels = labels.numpy()\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        decoded_preds = [pred.strip() for pred in decoded_preds]\n        decoded_labels = [[label.strip()] for label in decoded_labels]\n        all_preds.extend(decoded_preds)\n        all_labels.extend(decoded_labels)\n    \n    for i, metric in enumerate(metrics):\n        result[metrics_name[i]] = metric.compute(predictions=all_preds, references=all_labels)\n    \n    return result\n\n# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\", use_fast=False)\ntokenized_datasets = data_new.map(preprocess_function, batched=True, remove_columns=data_new[\"train\"].column_names)\nmodel = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)\ntf_train_dataset = model.prepare_tf_dataset( tokenized_datasets[\"train\"], collate_fn=data_collator, shuffle=True, batch_size=batch_size )\ntf_eval_dataset = model.prepare_tf_dataset( tokenized_datasets[\"test\"], collate_fn=data_collator, shuffle=False, batch_size=batch_size )\n\nscores[model_checkpoint] = compute_metrics(model)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T01:22:06.253318Z","iopub.execute_input":"2023-11-15T01:22:06.253824Z","iopub.status.idle":"2023-11-15T01:22:20.867817Z","shell.execute_reply.started":"2023-11-15T01:22:06.253789Z","shell.execute_reply":"2023-11-15T01:22:20.865418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data = {\"english_sentence\": [\"New English Sentence\"], \"hindi_sentence\": [\"Corresponding Hindi Sentence\"]}\ntokenized_new_data = preprocess_function(new_data)\n\nnew_data_embeddings = model(**tokenized_new_data)\n\nnew_data_representations = extract_embeddings(new_data_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\n# Load pre-trained mBART model and tokenizer\nmodel_name = \"facebook/mbart-large-50-many-to-many-mmt\"\ntokenizer = MBart50TokenizerFast.from_pretrained(model_name)\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\n\n# Example translation function with adjusted decoding parameters\ndef translate_text(input_text, model, tokenizer):\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n    translation_ids = model.generate(input_ids, max_length=150, num_beams=4, length_penalty=1.5, early_stopping=True, forced_bos_token_id=tokenizer.lang_code_to_id[\"hi-IN\"])\n    translated_text = tokenizer.decode(translation_ids[0], skip_special_tokens=True, )\n    return translated_text\n\n# Example usage with your dataset\nsample_data = data_new[\"train\"][0]\nenglish_sentence = sample_data['english_sentence']\n\n# Translate English to Hindi\nhindi_translation = translate_text(english_sentence, model, tokenizer)\n\n# Print results\nprint(\"English Sentence:\", english_sentence)\nprint(\"Hindi Translation:\", hindi_translation)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T02:16:44.163194Z","iopub.execute_input":"2023-11-15T02:16:44.163746Z","iopub.status.idle":"2023-11-15T02:17:11.357752Z","shell.execute_reply.started":"2023-11-15T02:16:44.163709Z","shell.execute_reply":"2023-11-15T02:17:11.355718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nfrom transformers import DataCollatorForSeq2Seq\nfrom datasets import Dataset, load_metric\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport numpy as np\n\nmax_length = 128  # Adjust as needed\ntgt_lang = \"hi_IN\"\n\n# Tokenizer\nmodel_checkpoint = \"facebook/mbart-large-50-many-to-many-mmt\"\ntokenizer = MBart50TokenizerFast.from_pretrained(model_checkpoint, src_lang=\"en_XX\", tgt_lang=tgt_lang)\n\n# Model\nmodel = MBartForConditionalGeneration.from_pretrained(model_checkpoint)\n\n# # Preprocessing function\n# def preprocess_function(data):\n#     inputs = [ex for ex in data[\"english_sentence\"]]\n#     targets = [ex for ex in data[\"hindi_sentence\"]]\n    \n#     # Add debugging information\n#     for idx, (input_text, target_text) in enumerate(zip(inputs, targets)):\n#         input_len = len(tokenizer(input_text)[\"input_ids\"])\n#         target_len = len(tokenizer(target_text)[\"input_ids\"])\n#         print(f\"Example {idx}: Input Length = {input_len}, Target Length = {target_len}\")\n\n#     model_inputs = tokenizer(inputs, text_target=targets, return_tensors=\"pt\", max_length=max_length, truncation=True)\ndef preprocess_function(example):\n    # Tokenize the source and target sentences\n    inputs = tokenizer(example['english_sentence'], return_tensors='tf', truncation=True, padding='max_length', max_length=64)\n    targets = tokenizer(example['hindi_sentence'], return_tensors='tf', truncation=True, padding='max_length', max_length=64)\n\n    # Return a dictionary with input and target sequences as lists\n    return {\n        'input_ids': [inputs['input_ids'][0]],\n        'attention_mask': [inputs['attention_mask'][0]],\n        'labels': [targets['input_ids'][0]],  # Assuming you're using a Seq2Seq model and want to generate target sequences\n    }\n\n# Training setup\nbatch_size = 4  # Adjust as needed\ntokenized_datasets = data_new.map(preprocess_function, batched=True, remove_columns=data_new[\"train\"].column_names)\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)\nimport tensorflow as tf\n\ndef generator(example):\n    # Convert the dictionary to a tuple of tensors\n    return (\n        {\n            'input_ids': tf.constant(example['input_ids']),\n            'attention_mask': tf.constant(example['attention_mask']),\n        },\n        tf.constant(example['labels'])\n    )\n\n# Convert tokenized datasets to TensorFlow datasets\ntf_train_dataset = tf.data.Dataset.from_generator(\n    lambda: map(generator, tokenized_datasets[\"train\"]),\n    output_signature=(\n        {\n            'input_ids': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n            'attention_mask': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n        },\n        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n    )\n)\n\ntf_eval_dataset = tf.data.Dataset.from_generator(\n    lambda: map(generator, tokenized_datasets[\"test\"]),\n    output_signature=(\n        {\n            'input_ids': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n            'attention_mask': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n        },\n        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n    )\n)\n\n# tf_train_dataset = model.prepare_tf_dataset(tokenized_datasets[\"train\"], collate_fn=data_collator, shuffle=True, batch_size=batch_size)\n# tf_eval_dataset = model.prepare_tf_dataset(tokenized_datasets[\"test\"], collate_fn=data_collator, shuffle=False, batch_size=batch_size)\nfrom transformers import MBartForConditionalGeneration, MBartTokenizer, DataCollatorForSeq2Seq\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n# Your existing code for model and tokenizer initialization\n\n# Training setup\nbatch_size = 4  # Adjust as needed\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)\n\n# Define training arguments and initialize Trainer\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    save_total_limit=2,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    data_collator=data_collator,\n#     prediction_loss_only=True,\n)\n\n# Start training\ntrainer.train()\n\n# Metrics\nmetrics = load_metric(\"sacrebleu\")\nmetrics_name = \"sacrebleu\"\n\n# Training loop\n@tf.function(jit_compile=True)\ndef generate_with_xla(model, batch):\n    return model.generate(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], max_length=128)\n\ndef compute_metrics(model):\n    all_preds = []\n    all_labels = []\n    result = {}\n    for batch, labels in tqdm(tf_eval_dataset):\n        predictions = generate_with_xla(model, batch)\n        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n        labels = labels.numpy()\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        decoded_preds = [pred.strip() for pred in decoded_preds]\n        decoded_labels = [[label.strip()] for label in decoded_labels]\n        all_preds.extend(decoded_preds)\n        all_labels.extend(decoded_labels)\n\n    result[metrics_name] = metrics.compute(predictions=all_preds, references=all_labels)\n    return result\n\n# Training\nmodel_checkpoint = \"facebook/mbart-large-50-many-to-many-mmt\"\nscores = {}\n\ntry:\n    scores[model_checkpoint] = compute_metrics(model)\nexcept Exception as e:\n    print(f\"Error processing model '{model_checkpoint}': {str(e)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T01:58:22.651915Z","iopub.execute_input":"2023-11-15T01:58:22.652557Z","iopub.status.idle":"2023-11-15T02:01:47.555945Z","shell.execute_reply.started":"2023-11-15T01:58:22.652517Z","shell.execute_reply":"2023-11-15T02:01:47.553615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(scores)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T22:11:50.168095Z","iopub.status.idle":"2023-11-14T22:11:50.168845Z","shell.execute_reply.started":"2023-11-14T22:11:50.168473Z","shell.execute_reply":"2023-11-14T22:11:50.168515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(text):\n    text = eng_tokenizer.texts_to_sequences([text])\n    text = tf.keras.preprocessing.sequence.pad_sequences(text, maxlen=ENCODER_LEN, \n                                                                   padding='post', truncating='post')\n\n    encoder_input = tf.expand_dims(text[0], 0)\n\n    decoder_input = [hind_tokenizer.word_index['<sos>']]\n    output = tf.expand_dims(decoder_input, 0)\n    \n    for i in range(DECODER_LEN):\n        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n\n        predictions, attention_weights = transformer(\n            encoder_input, \n            output,\n            False,\n            enc_padding_mask,\n            combined_mask,\n            dec_padding_mask\n        )\n\n        predictions = predictions[: ,-1:, :]\n        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n\n        if predicted_id == hind_tokenizer.word_index['<eos>']:\n            return tf.squeeze(output, axis=0), attention_weights\n\n        output = tf.concat([output, predicted_id], axis=-1)\n\n    return tf.squeeze(output, axis=0), attention_weights\n\ndef translate(eng_text):\n    hind_text = evaluate(text=eng_text)[0].numpy()\n    hind_text = np.expand_dims(hind_text[1:], 0)  \n    return hind_tokenizer.sequences_to_texts(hind_text)[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:16:33.850479Z","iopub.execute_input":"2023-11-20T08:16:33.851639Z","iopub.status.idle":"2023-11-20T08:16:33.862650Z","shell.execute_reply.started":"2023-11-20T08:16:33.851602Z","shell.execute_reply":"2023-11-20T08:16:33.861568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv\")\ntrain_df.drop(['source'],axis=1,inplace=True)\nmask = (train_df['english_sentence'].str.len()>20) & (train_df['english_sentence'].str.len()<200)\ntrain_df = train_df.loc[mask]\ntrain_df = train_df.sample(64000, random_state=1)\ntrain_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:17:39.051734Z","iopub.execute_input":"2023-11-20T08:17:39.052443Z","iopub.status.idle":"2023-11-20T08:17:39.959325Z","shell.execute_reply.started":"2023-11-20T08:17:39.052409Z","shell.execute_reply":"2023-11-20T08:17:39.958344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\n\neng = train_df['english_sentence']\nhind = train_df['hindi_sentence']\neng = eng.apply(lambda x: \"<SOS> \" + str(x) + \" <EOS>\")\nhind = hind.apply(lambda x: \"<SOS> \"+ x + \" <EOS>\")\n\nfilters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\noov_token = '<unk>'\neng_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = filters, oov_token=oov_token)\nhind_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = filters, oov_token=oov_token)\neng_tokenizer.fit_on_texts(eng)\nhind_tokenizer.fit_on_texts(hind)\ninputs = eng_tokenizer.texts_to_sequences(eng)\ntargets = hind_tokenizer.texts_to_sequences(hind)\n\nENCODER_VOCAB = len(eng_tokenizer.word_index) + 1\nDECODER_VOCAB = len(hind_tokenizer.word_index) + 1\nprint(ENCODER_VOCAB, DECODER_VOCAB)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:20:54.995634Z","iopub.execute_input":"2023-11-20T08:20:54.996483Z","iopub.status.idle":"2023-11-20T08:21:01.896187Z","shell.execute_reply.started":"2023-11-20T08:20:54.996446Z","shell.execute_reply":"2023-11-20T08:21:01.895147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ENCODER_LEN = 100\nDECODER_LEN = 100\nBATCH_SIZE = 64\nBUFFER_SIZE = BATCH_SIZE*4","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:21:01.897809Z","iopub.execute_input":"2023-11-20T08:21:01.898087Z","iopub.status.idle":"2023-11-20T08:21:01.903066Z","shell.execute_reply.started":"2023-11-20T08:21:01.898061Z","shell.execute_reply":"2023-11-20T08:21:01.902172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_masks(inp, tar):\n    enc_padding_mask = create_padding_mask(inp)\n    dec_padding_mask = create_padding_mask(inp)\n\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n  \n    return enc_padding_mask, combined_mask, dec_padding_mask","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:21:01.904147Z","iopub.execute_input":"2023-11-20T08:21:01.904524Z","iopub.status.idle":"2023-11-20T08:21:01.914442Z","shell.execute_reply.started":"2023-11-20T08:21:01.904490Z","shell.execute_reply":"2023-11-20T08:21:01.913539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_angles(position, i, d_model):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n    return position * angle_rates\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(\n        np.arange(position)[:, np.newaxis],\n        np.arange(d_model)[np.newaxis, :],\n        d_model\n    )\n\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\ndef create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  \n\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n\n    output = tf.matmul(attention_weights, v)\n    return output, attention_weights\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n        \n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n        \n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n        output = self.dense(concat_attention)\n            \n        return output, attention_weights\n    \ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(dff, activation='relu'),\n        tf.keras.layers.Dense(d_model)\n    ])\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n\n        return out2\n\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n    \n    \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)\n\n        ffn_output = self.ffn(out2)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)\n\n        return out3, attn_weights_block1, attn_weights_block2\n\nclass Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n    \n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n    \n        return x\n    \nclass Decoder(tf.keras.layers.Layer):\n        \n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n    \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n\n            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n    \n        return x, attention_weights\n    \n\nclass Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n    \n    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inp, training, enc_padding_mask)\n\n        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)\n\n        return final_output, attention_weights\n\nnum_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\ndropout_rate = 0.1\nEPOCHS = 10","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:21:01.916527Z","iopub.execute_input":"2023-11-20T08:21:01.916840Z","iopub.status.idle":"2023-11-20T08:21:01.961344Z","shell.execute_reply.started":"2023-11-20T08:21:01.916815Z","shell.execute_reply":"2023-11-20T08:21:01.960410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = Transformer(\n    num_layers=num_layers,\n    d_model=d_model,\n    num_heads=num_heads,\n    dff=dff,\n    input_vocab_size=ENCODER_VOCAB,\n    target_vocab_size=DECODER_VOCAB,\n    pe_input=1000,\n    pe_target=1000,\n    rate=dropout_rate)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:21:01.962379Z","iopub.execute_input":"2023-11-20T08:21:01.962675Z","iopub.status.idle":"2023-11-20T08:21:02.149991Z","shell.execute_reply.started":"2023-11-20T08:21:01.962633Z","shell.execute_reply":"2023-11-20T08:21:02.149157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n    \n#     def __call__(self, step):\n#         arg1 = tf.math.rsqrt(step)\n#         arg2 = step * (self.warmup_steps ** -1.5)\n\n#         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n    \n    def __call__(self, step):\n        step = tf.cast(step, dtype=tf.float32)  # Cast step to float32\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:32:53.315361Z","iopub.execute_input":"2023-11-20T08:32:53.315732Z","iopub.status.idle":"2023-11-20T08:32:53.323217Z","shell.execute_reply.started":"2023-11-20T08:32:53.315701Z","shell.execute_reply":"2023-11-20T08:32:53.322256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:32:54.234999Z","iopub.execute_input":"2023-11-20T08:32:54.235762Z","iopub.status.idle":"2023-11-20T08:32:54.257659Z","shell.execute_reply.started":"2023-11-20T08:32:54.235726Z","shell.execute_reply":"2023-11-20T08:32:54.256663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = \"/kaggle/input/nlp-project-datset-assist/ckpt-2.index\"\n\nckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\ncheckpoint_path = \"checkpoints\"\n\n# Restore the latest checkpoint\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print('Latest checkpoint restored!!')\nelse:\n    print(\"Checkpoint not found. Training from scratch or provide the correct checkpoint path.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:32:55.922798Z","iopub.execute_input":"2023-11-20T08:32:55.923578Z","iopub.status.idle":"2023-11-20T08:32:55.935632Z","shell.execute_reply.started":"2023-11-20T08:32:55.923545Z","shell.execute_reply":"2023-11-20T08:32:55.934638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate(\"That politics , in retrospect , was rooted in a false ideology\")","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:21:02.151321Z","iopub.execute_input":"2023-11-20T08:21:02.152007Z","iopub.status.idle":"2023-11-20T08:21:23.321476Z","shell.execute_reply.started":"2023-11-20T08:21:02.151972Z","shell.execute_reply":"2023-11-20T08:21:23.320466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}